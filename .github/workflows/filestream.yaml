# .github/workflows/filestream.yaml
name: Stream Datasets into Azure Blob

on:
  # Currently set to manual startup.
  workflow_dispatch:

jobs:
  prepare-matrix:
    runs-on: ubuntu-latest

    outputs:
      matrix: ${{ steps.build_matrix.outputs.matrix }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Build JSON Array from File
        id: build_matrix
        shell: bash
        run: |
          set -euo pipefail

          # Initialize file path.
          FILE_LIST=".github/workflows/datasets.txt"

          # Exit if file not found.
          if [ ! -f "$FILE_LIST" ]; then
            echo "datasets.txt not found at $FILE_LIST"
            echo 'matrix={"include":[]}' >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Initialize parsing variables.
          i=0
          entries=""
          separator=""

          # Parse through each line in the file.
          while read -r url blob_path; do
            # Skip empty and commented lines.
            if [ -z "$url" ] || [[ "$url" == \#* ]]; then
              continue
            fi

            # Continue to next line (update index).
            i=$((i + 1))

            # Append batch number object {"batch":i} to the JSON array.
            entries="${entries}${separator}{\"batch\":$i}"
            separator=","
          done < "$FILE_LIST"

          # Exit if file has no active lines to read.
          if [ "$i" -eq 0 ]; then
            echo "No active datasets found in $FILE_LIST."
            echo 'matrix={"include":[]}' >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Export JSON array into matrix.
          json="{\"include\":[${entries}]}"
          echo "Matrix JSON: $json"
          echo "matrix=$json" >> "$GITHUB_OUTPUT"

  stream-datasets:
    needs: prepare-matrix
    runs-on: ubuntu-latest

    # Matrix strategy for streaming large quantities of datasets autonomously.
    strategy:
      matrix: ${{ fromJson(needs['prepare-matrix'].outputs.matrix) }}
      # Stream files sequentially to prevent overloading the server with requests.
      max-parallel: 1

    # Intialize workflow variables with GitHub Actions secrets
    env:
      STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      CONTAINER: ${{ secrets.AZURE_CONTAINER }}
      SAS: ${{ secrets.ADLS_SAS }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install AzCopy
        run: |
          set -euo pipefail

          AZCOPY_TAR=azcopy_linux_amd64.tar.gz
          curl -sSL https://aka.ms/downloadazcopy-v10-linux -o "$AZCOPY_TAR"
          tar -xzf "$AZCOPY_TAR"
          sudo cp ./azcopy_linux_amd64_*/azcopy /usr/local/bin/

          azcopy --version

      - name: Stream Batch File
        env:
          BATCH: ${{ matrix.batch }}
        run: |
          set -euo pipefail

          # Initialize file path and current batch.
          FILE_LIST=".github/workflows/datasets.txt"
          BATCH="${BATCH:-1}"

          # Exit if file not found.
          if [ ! -f "$FILE_LIST" ]; then
            echo "datasets.txt not found at $FILE_LIST"
            exit 1
          fi

          # Announce parameters for current job.
          echo "Using file list: $FILE_LIST"
          echo "Dataset index: $BATCH"

          # Parse for the N-th active (non-empty, uncommented) line from file.
          line=$(awk -v target="$BATCH" '
            /^[[:space:]]*#/ {next}   # skip comments
            NF == 0 {next}            # skip empty lines
            {count++}
            count == target {print; exit}
          ' "$FILE_LIST")

          # Exit if corresponding file index not found.
          if [ -z "$line" ]; then
            echo "No dataset with index $BATCH."
            exit 0
          fi

          # Extract URL and blob path from the corresponding line.
          url=$(printf '%s\n' "$line" | awk '{print $1}')
          blob_path=$(printf '%s\n' "$line" | awk '{print $2}')

          # Announce file upload commencing.
          echo "Uploading [$BATCH] $url as $blob_path ..."

          # Create/empty log for failed uploads.
          : > failed_uploads.txt

          # Use AzCopy to stream source file through stdout and into blob. Exit if upload failed.
          if curl -L "$url" \
            | azcopy copy \
                "https://${STORAGE_ACCOUNT}.blob.core.windows.net/${CONTAINER}/$blob_path?$SAS" \
                --from-to=PipeBlob \
                --check-length=true; then
            echo "✅ SUCCESS [$BATCH]: $blob_path"
          else
            echo "❌ FAILED [$BATCH]: $blob_path" | tee -a failed_uploads.txt
            exit 1
          fi

      - name: Upload Failure List as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: failed-uploads-batch-${{ matrix.batch }}
          path: failed_uploads.txt
          if-no-files-found: ignore
