{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cefc33-8dbc-46b8-aa14-417b8bd74a19",
   "metadata": {},
   "source": [
    "# Tutorial: Star-formation convolution\n",
    "This notebook serves to explain how to use the basic features of the star-formation convolution code. We provide a step-by-step explanation of the convolution, including examples of code. More advanced features and fleshed-out use-cases are covered in other notebooks.\n",
    "\n",
    "First, we cover some of the concepts of convolving star-formation rates with population-synthesis results in [Background on convolution](#Background-on-convolution), and then we dive into the technical details of how to use the star-formation rate convolution code in [Convolution with Synthetic Stellar Pop Convolve (SSPC)](#Convolution-with-Synthetic-Stellar-Pop-Convolve-(SSPC)).\n",
    "\n",
    "Similar codes that are on the market are:\n",
    "- Cogsworth (Wagg) https://arxiv.org/abs/2409.04543\n",
    "- SynthPop Kluter et al https://arxiv.org/abs/2411.18821\n",
    "- Galaxia https://galaxia.sourceforge.net/Galaxia3pub.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19423fcb-ddf8-4026-bd2d-41bbccb519ad",
   "metadata": {},
   "source": [
    "## Background on convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4505ca1-d5f1-422d-b919-3e1a779827bb",
   "metadata": {},
   "source": [
    "With stellar population-synthesis codes one often calculates statistics of populations of stellar systems, but these results often are expressed in quantities 'per formed stellar mass', i.e. these results need to be combined with a star formation rate to get an actual predicted number of stellar systems. Some useful (recent) literature on the concept of convolution of starformation rates with population-synthesis results, analytic descriptions and formulae related to the convolution, or technical implementations, are: \n",
    "\n",
    "- [Izzard & Halabi. 2018](https://ui.adsabs.harvard.edu/abs/2018arXiv180806883I/abstract)\n",
    "- [Neijssel et al. 2019](https://doi.org/10.1093/mnras/stz2840)\n",
    "- [Broekgaarden et al. 2021](https://doi.org/10.1093/mnras/stab2716) (sec 2.2)\n",
    "- [Broekgaarden et al. 2022](https://doi.org/10.1093/mnras/stac1677)\n",
    "- [van Son et al. 2022](https://doi.org/10.3847/1538-4357/ac64a3)\n",
    "- [van Son et al. 2023](https://iopscience.iop.org/article/10.3847/1538-4357/acbf51)\n",
    "- [Riley et al. 2022](https://ui.adsabs.harvard.edu/abs/2022ApJS..258...34R) section 6.3 to 6.6\n",
    "- [Hendriks et al. 2023](https://ui.adsabs.harvard.edu/abs/2023MNRAS.526.4130H/abstract)\n",
    "- [Chruślińska 2022](https://ui.adsabs.harvard.edu/abs/2022arXiv220610622C/abstract)\n",
    "- [Chruślińska et al 2019a](https://doi.org/10.1093/mnras/sty3087) \n",
    "- [Chruślińska et al 2019b](https://ui.adsabs.harvard.edu/abs/2019MNRAS.488.5300C) \n",
    "- [Chruślińska et al. 2018](https://ui.adsabs.harvard.edu/abs/2018MNRAS.474.2937C)\n",
    "- [Chruślińska et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A&A...636A..10C/abstract)\n",
    "- [Breivik et al ](https://iopscience.iop.org/article/10.3847/1538-4357/ab9d85/pdf)\n",
    "\n",
    "\n",
    "\n",
    "Our convolution code aims to perform these calculations\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_{\\mathrm{\\bhbh}}(z_{\\mathrm{merge}},\\ \\zeta) =\n",
    "\\int_{Z_{\\mathrm{min}}}^{Z_{\\mathrm{max}}} dZ\n",
    "\\int_{0}^{t^{*}_{\\mathrm{first\\, SFR}}-t^{*}_{\\mathrm{merge}}}dt_{\\mathrm{delay}}\\\\\n",
    "\\mathcal{N}_{\\mathrm{form}}(Z,\\,t_{\\mathrm{delay}},\\,\\zeta)\n",
    "\\times \\mathrm{SFR}(Z,\\,z_{\\mathrm{birth}}).\n",
    "$$\n",
    "\n",
    "When the starformation is expressed as a function of redshift, we perform a set of conversions between event-redshift and birth-redshift, using the delay time of the system/ensemble\n",
    "\n",
    "$$\n",
    "R_{\\mathrm{merge},\\,i},\\ z_{\\mathrm{merge}} = p_{i}\\,\\times\\,f_{\\mathrm{bin}}\\,\\times\\,\\frac{\\mathrm{SFR}(z_{\\mathrm{birth},\\,i},\\ Z_{i})\\,\\times\\,dZ_{i}}{\\left<M\\right>},\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7320e-62a8-4a35-87ef-48e0d99ef49a",
   "metadata": {},
   "source": [
    "## Convolution with Synthetic Stellar Pop Convolve (SSPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179106f-ae05-4aba-8262-b3cb7335f31d",
   "metadata": {},
   "source": [
    "At a high level, the convolution code has three stages. \n",
    "\n",
    "- setup: sequential preparation of the hdf5 file. A set of steps including putting the event and ensemble (population) data in the correct structure and format, providing settings used to generate the population data, generating the star-formation rate grid, setting up the output file etc.\n",
    "- convolution: multi-processed multiplication of events or ensembles with star-formation rates (densities). The convolution is handled by finding the star-formation rate bin in which a system/ensemble falls if we want the system/ensemble with delay time $t_{\\mathrm{delay}}$ to have an event at some target lookback time/redshift $t_{\\mathrm{target\\, lookback}}$. \n",
    "- storage: sequential storage of all the results into the output hdf5 file. The results of the convolution at each target time are stored in temporary files before they are written into the output hdf5 file.\n",
    "\n",
    "To convolve population-synthesis results we need to go through the following steps and sections:\n",
    "\n",
    "- [Construction of the input file](#Construction-of-the-input-file): we first need to construct the convolution input file appropriately. This means storing the results in a certain structure in an HDF5 file.\n",
    "- [Global configuration of the convolution](#General-configuration-of-the-convolution): we then need to provide a global configuration for the convolution, which includes specifying the star-formation history, as well as specifics about the time-stepping. For a complete overview of the configuration options see [this section](#descriptions-of-configuration-options)\n",
    "- [Specific convolution instructions](#Specific-convolution-instructions): after the global configuration we need to provide specific convolution instructions, which contain specifics of which dataset is convolved, which column-names or ensemble-depths are associated with the necessary quantities for convolution (i.e. delay time, metallicity, yield), and other additional things like extra weights (e.g. detection probabilities).\n",
    "- [Running the convolution](#Running-the-convolution): when the above steps are finished we can convolve the data.\n",
    "- [Reading the output file](#Reading-the-output-file): After the convolution, the data is stored in the output hdf5 file. There is a particular structure to this output data, which is explained in this section.\n",
    "\n",
    "At the end of the notebook we have a [section that describes all the global configuration options](#descriptions-of-configuration-options), as well as a [section that contains a complete convolution script](#Complete-convolution-script)\n",
    "\n",
    "The codebase for the convolution functionality resides in `syntheticstellarpopconvolve/`, and we suggest having a look through the code to make yourself familiar with how things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd71f61f-4f42-4e1e-a88a-2cd8167a347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "from syntheticstellarpopconvolve import convolve, default_convolution_config, default_convolution_instruction\n",
    "from syntheticstellarpopconvolve.general_functions import temp_dir\n",
    "\n",
    "\n",
    "TMP_DIR = temp_dir(\"notebooks\", \"notebook_convolution\", clean_path=True)\n",
    "VERBOSITY = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201db88d-cf02-435e-90db-080b836751a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Construction of the output file\n",
    "To convolve the data we first need to construct an input file that contains the correct information, and stores the input data in a specific way, and includes some extra information like the population settings that were used to generate the data. The file format we use for the convolution output files are hdf5 files. \n",
    "\n",
    "Lets construct the file using the utility function `generate_boilerplate_outputfile`, which creates the file as well as certain data groups within this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53de14c0-c443-417c-b841-efd17d7ed78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "from syntheticstellarpopconvolve.general_functions import generate_boilerplate_outputfile\n",
    "     \n",
    "# create file\n",
    "output_hdf5_filename = os.path.join(TMP_DIR, 'input_hdf5.h5')\n",
    "generate_boilerplate_outputfile(output_hdf5_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b3b7e-05ed-4346-afce-2634ea8d7b6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supported data formats and storing data\n",
    "After creating the input file itself we need to fill it with the data we want to convolve. We show how to do that in the following section.\n",
    "\n",
    "While this code is agnostic as to where the data comes from, i.e. any population synthesis code that generates the data in the correct format would be suitable, we indicate methods to generate this data with `binary_c` / `binary_c-python`.\n",
    "\n",
    "Note that since we do not provide any methods to filter the data during the convolution, what is provided will be convolved. If a set of events contains more than you need (e.g. you're only interested in systems that form SN1a) then its best to filter out the rest beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98d601-4738-4f11-8719-0a04f53a5f94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### event-based data\n",
    "\n",
    "Event-based data is stored in pandas dataframes, and has the form of a rectangular, line-by-line, data format.\n",
    "\n",
    "An example of event-based data looks like this:\n",
    "\n",
    "```\n",
    "uuid    event_type  zams_mass_1 zams_mass_2 zams_orbital_period [...]\n",
    "B2D5742C-C96C-42EB-8A44-754E4D9E867F    RLOF    29.9112 3.8264  6.58795 [...]\n",
    "A2339A8F-12A4-4F8F-B594-99F22B00360C    RLOF    11.8937 4.52266 6.58795 [...]\n",
    "[...]\n",
    "```\n",
    "\n",
    "With `binary_c-python` this type of data can be generated through the options explained in [the event-based logging notebook](https://binary_c.gitlab.io/binary_c-python/event_based_logging_descriptions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905eb13c-225b-4cd7-bbcf-1cbb8165b4ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ensemble-based data\n",
    "\n",
    "Ensemble-based data is stored as nested dictionaries. An example of ensemble-based data looks like this:\n",
    "\n",
    "``` python\n",
    "\"Xyield\": {\n",
    "    \"time\": {\n",
    "        \"-0.1\": {\n",
    "            \"source\": {\n",
    "                \"Wind\": {\n",
    "                    \"isotope\": {\n",
    "                        \"Al27\": 1.3202421292393783e-08,\n",
    "                        \"Ar36\": 1.7624018781546946e-08,\n",
    "                        \"Ar38\": 3.502033439864038e-09,\n",
    "                        \"Ar40\": 5.758546201573555e-12,\n",
    "                        \"B10\": 2.4295643555965993e-13,\n",
    "                        \"B11\": 1.0109986571758494e-12,\n",
    "                        \"Be9\": 3.7843822119497306e-14,\n",
    "                        [...]\n",
    "                        }\n",
    "                    }\n",
    "                [...]\n",
    "                }\n",
    "            }\n",
    "        [...]\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "With `binary_c-python` this type of data can be generated through the options explained in [the ensemble-data logging notebook](https://binary_c.gitlab.io/binary_c-python/examples/notebook_ensembles.html).\n",
    "\n",
    "To use this type of data, however, one must first transform it to a different shape. In particular one must `inflate` the ensemble, turning it from a nested dictionairy to a rectangular data format. How to do so is covered in XXX (TODO: refer to notebook).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45df85-701d-4123-b428-d0e9e96d42da",
   "metadata": {},
   "source": [
    "#### Storing the data\n",
    "\n",
    "Data is supposed to be stored in an hdf5 file under a particular key structure: `input_data/<input_data_name>` where `<input_data_name>` is the name associated with that particular set of event data (like `RLOF` events). `<input_data_name>` is something the user can decide for themselves, and will be used to retrieve the data during the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a14dcc-f17a-4d0c-8268-24be5e85f7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load as RLOF event-data as pandas df\n",
    "example_RLOF_events_filename = pkg_resources.resource_filename(\n",
    "    \"syntheticstellarpopconvolve\", 'example_data/example_RLOF_event_data.dat'\n",
    ")\n",
    "RLOF_df = pd.read_csv(example_RLOF_events_filename, header=0, sep=\"\\s+\")\n",
    "\n",
    "# store the dataframe in the hdf5file\n",
    "RLOF_df.to_hdf(\n",
    "    output_hdf5_filename, key=\"input_data/{}\".format(\"RLOF\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243798ff-8cc0-4771-9b38-cc7dab8c3dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration of the convolution\n",
    "After setting up the input file for the convolution, we need to provide a configuration for the convolution.\n",
    "\n",
    "for a complete list of configuration options, please see the [descriptions of configuration options](#descriptions-of-configuration-options) section and the [online documentation for config options](https://synthetic-stellar-pop-convolve.readthedocs.io/en/latest/default_convolution_config.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8c6af0-7ec1-4b20-9783-3afb8f9f6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "convolution_config = copy.copy(default_convolution_config)\n",
    "convolution_config['output_filename'] = output_hdf5_filename\n",
    "convolution_config['tmp_dir'] = TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df48309-63ab-4874-a055-7ea74137ac00",
   "metadata": {},
   "source": [
    "### Configuring the starformation rate\n",
    "\n",
    "The next step is to provide the code with a star-formation rate that is used to convolve the data with. We explain the details of this in a different notebook, to avoid this one from becoming too large \n",
    "\n",
    "We remain rather agnostic as to where these come from and how they are constructed, but we do provide some distributions that you can use to construct them. These are available in `syntheticstellarpopconvolve.starformation_rate_distributions` and `syntheticstellarpopconvolve.metallicity_distributions`.\n",
    "\n",
    "**TODO: refer to other sectin for SFR dicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd81cb3f-2d61-4e47-8e7e-deeae210098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up SFR dict\n",
    "convolution_config[\"SFR_info\"] = {\n",
    "    'lookback_time_bin_edges': np.arange(0, 10, 1) * u.yr,\n",
    "    'starformation_rate_array': np.arange(0,  9) ** 2 * u.Msun / u.yr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb21ac0-e0c8-43ca-bde6-96a7444fc4a8",
   "metadata": {},
   "source": [
    "## Configuring convolution bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8f8af6-8ee2-48a0-84f6-47f845dda9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set convolution time bins\n",
    "convolution_config[\"convolution_lookback_time_bin_edges\"] = np.arange(0, 3, 1) * u.yr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b36034-0638-468b-abf5-1c744f310581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specific convolution instructions\n",
    "After configuring the global convolution behaviour, we need to instruct the code how to convolve specific datasets (possibly with different data structures). This is done through the `convolution_instructions`. This entry in the config needs to contain a list of dictionaries that each contain a set of instructions for the convolution. It is possible to provide several dictionaries, which will be handled sequentially in the code.\n",
    "\n",
    "an example would be as such:\n",
    "\n",
    "```python\n",
    "convolution_config['convolution_instructions'] = [\n",
    "    {\n",
    "        'input_data_name': 'RLOF',\n",
    "        'output_data_name': 'RLOF_rate',\n",
    "        ... (see below)\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "Each convolution_instruction dict in that list needs to contain at least the following entries:\n",
    "\n",
    "- `input_data_name`: the name of the dataset that is to be convolved. See the [construction of the input file](#Construction-of-the-input-file)\n",
    "- `output_data_name`: the output name for this convolution. This gives an additional name to output, allowing us to convolve the same input dataset multiple times with different configurations.\n",
    "- `data_column_dict`: Dictionary containing mapping of input-data column names to those required by the convolution code. See below for more details \n",
    "\n",
    "And the following optional entries:\n",
    "\n",
    "- `post_convolution_function`: A function that will be called after the convolution step. **TODO refer to specific section**\n",
    "\n",
    "Then, depending on the input type, several other entries must (or may) be provided to the `convolution_instruction` entry. Below we list these according to the input-data type. More importantly, there are some restrictions as well\n",
    "\n",
    "| convolution type | General support | Time-type support |\n",
    "| --- | --- | --- |\n",
    "| integration| Yes | lookback time & redshift |\n",
    "| sampling| Yes | | |\n",
    "\n",
    "#### event-based data\n",
    "event-based data (i.e. line-based, with each line containing an event, see event-based logging [documentation](https://binary_c.gitlab.io/binary_c-python/event_based_logging_descriptions.html) and [notebook](https://binary_c.gitlab.io/binary_c-python/examples/notebook_event_based_logging.html) on how to generate that with `binary_c-python`) needs information about which columns to use. These should be provided through a dictionary called `data_column_dict`.\n",
    "\n",
    "Within this dictionary, the following entries must be present\n",
    "\n",
    "- `delay_time`: column name associated with the delay time of the event (time between the birth of the system to the occurrence of the event). Assumed to be expressed in years.\n",
    "- `normalized_yield`: column name associated with the yield rate of the systems. This column gets multiplied by the star formation rate (density), and so is expected to already be converted into something per solar mass formed into stars.\n",
    "\n",
    "optionally, one can add\n",
    "\n",
    "- `metallicity`: column name associated with the metallicity of the systems.\n",
    "\n",
    "As well as other columns that can be used in the extra_weight calculation (see TODO)\n",
    "\n",
    "The `data_column_dict` values should either be of string type or a dictionary that contains (some of) the following items:\n",
    "\n",
    "- `column_name`: integer indicating the depth of the layer that contains this data\n",
    "\n",
    "As an example:\n",
    "\n",
    "```python\n",
    "convolution_config['convolution_instructions'] = [\n",
    "    {\n",
    "        ...,\n",
    "        'data_column_dict': {\n",
    "            # required\n",
    "            'delay_time': 'initial_time',\n",
    "            'normalized_yield': 'probability',\n",
    "            # optional*\n",
    "            'metallicity': 'metallicity',\n",
    "            # optional**\n",
    "            'mass_1': 'RLOF_initial_mass_accretor',\n",
    "            'mass_2': 'RLOF_initial_mass_donor',\n",
    "        },\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "additional configuration of the convolution-instructions are covered in the advanced-usage notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a26cb-5a3f-4b4e-9b8d-76ede4e94da0",
   "metadata": {},
   "source": [
    "### Post-convolution processing function\n",
    "The convolution instruction can be supplemented with a post-convolution function. This can be used for a range of reasons, e.g. filtering of the data to only include particular systems, providing additional detection-probability weighting to go from intrinsic rates to predicted observed rates, further evolution of sampled systems to the present-day through gravitational-wave radiation, orbit integration (including kicks), etc. \n",
    "\n",
    "The user can provide the `convolution_instructions` dictionary with an additional function that performs some additional calculations. This is done as follows. The extra_weights_function should be stored in the `convolution_instructions` as `convolution_instructions['post_convolution_function']`. This function should return an array of additional weights.\n",
    "\n",
    "The arguments to this function do not require any specific parameter, but the convolution code internally allows the following arguments to be passed to the function:\n",
    "\n",
    "- `config`: general configuration dictionary\n",
    "- `time_value`: the time value of the current time-bin (can be redshift as well, but must be named `time_value`.\n",
    "- `convolution_instruction`: convolution instruction dictionary.\n",
    "- `job_dict`: dictionary containing current job information\n",
    "- `sfr_dict`: dictionary containing the star formation rate dictionary\n",
    "- `convolution_results`: dictionary containing the results of the current convolution step. This one is rather important. **TODO: likely is required.**\n",
    "- `data_dict`: dictionary that contains the data. This will be filled with the keys and data that either `data_layer_dict` or `data_column_dict` instructs the convolution code to use.\n",
    "- `**convolution_instruction[\"post_convolution_function_extra_parameters\"]`: additional parameters and values that the user can provide.\n",
    "\n",
    "Which of these arguments gets passed depends on the arguments to the `post_convolution_function`, which are automatically recognized and passed to the function.\n",
    "\n",
    "This function should return an updated `result_dict` dictionary, where the results are updated accordingly. Apart from updating the `yield` field, which contains the conventional convolution results, one (in some cases, see below) can add extra fields to this dictionary, like its position in space. \n",
    "\n",
    "Depending on the type of convolution, and the data type, the shape (length) of the entries in this dictionary can have different requirements. \n",
    "\n",
    "This is to maintain correct data-integrity.\n",
    "- convolution by integration: length of arrays has to be the same before as after\n",
    "- convolution by sampling: no restrictions\n",
    "\n",
    "It is also possible to return several `result_dict`s, but these need to be supplied with a `name` to identify and store the data properly. An example of a use-case for returning several `return_dict`s is when convolving event-based data by sampling, integrating the sampled systems forward in time and evolving the orbit of the binary system. When a supernova kick unbinds the binary, the system will turn into 2 individual stars, which requires storing more data to track both stars. In this case it would make sense to return one `result_dict` that contains bound binary systems, and one `result_dict` containing information for both unbound stars. data-integrity requirements outlined above still count. \n",
    "\n",
    "An example of a post-convolution function call is as follows:\n",
    "\n",
    "```python\n",
    "def post_convolution_function_detection_probability(config, time_value, data_dict, min_detection_probability):\n",
    "    # just an example, this is not how detection probability is calculated\n",
    "    detection_probability = data_dict['mass_1'] * data_dict['mass_2'] * time_value\n",
    "    \n",
    "    detection_probability[detection_probability<min_detection_probability] = 0\n",
    "\n",
    "    return detection_probability\n",
    "\n",
    "# \n",
    "convolution_config['convolution_instructions'] = [\n",
    "    {\n",
    "        'input_data_name': 'RLOF',\n",
    "        'output_data_name': 'RLOF_rate',\n",
    "        'data_column_dict': {\n",
    "            # required\n",
    "            'delay_time': 'initial_time',\n",
    "            'normalized_yield': 'probability',\n",
    "            # optional*\n",
    "            'metallicity': 'metallicity',\n",
    "            # optional**\n",
    "            'mass_1': 'RLOF_initial_mass_accretor',\n",
    "            'mass_2': 'RLOF_initial_mass_donor',\n",
    "        },\n",
    "        'post_convolution_function': post_convolution_function_detection_probability,\n",
    "        'post_convolution_function_extra_parameters': {'min_detection_probability': 1e-6}\n",
    "    },\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "PS: the use of `post_convolution_function_extra_parameters` is somewhat unnecessary, as one can either hardcode the additional parameters within the function, or make use of the `partial` method (see https://www.geeksforgeeks.org/partial-functions-python/)\n",
    "\n",
    "Some useful resources:\n",
    "- https://arxiv.org/abs/2404.16930 for GW detection\n",
    "- https://gaiaunlimited.readthedocs.io/ for gaia selection functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0089405-fb77-4c89-ad26-754659430402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running the convolution\n",
    "Before convolve your data, please make sure to follow the above steps. \n",
    "\n",
    "Then, to convolve the data, we should provide a correct configuration dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f03a398-6b62-40e8-92d5-6874acd2440f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['config', 'input_data', 'output_data']>\n"
     ]
    }
   ],
   "source": [
    "convolution_config['convolution_instructions'] = [\n",
    "    {\n",
    "        **default_convolution_instruction,\n",
    "        'input_data_name': 'RLOF',\n",
    "        'output_data_name': 'RLOF_rate',\n",
    "        'data_column_dict': {\n",
    "            # required\n",
    "            'delay_time': 'initial_time',\n",
    "            'normalized_yield': 'probability',\n",
    "            # # optional*\n",
    "            # 'metallicity': 'metallicity',\n",
    "            # optional**\n",
    "            'mass_1': 'initial_mass_accretor',\n",
    "            'mass_2': 'initial_mass_donor',\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# convolve\n",
    "convolve(config=convolution_config)\n",
    "\n",
    "# \n",
    "with h5py.File(output_hdf5_filename, 'r') as output_hdf5_file:\n",
    "    print(output_hdf5_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdae42-d020-465e-a9e2-3dcd49b85e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading and understanding the output\n",
    "The results of the convolution are stored in the `output_data` group. The tree structure of this group, for a given convolution instruction result, is as:\n",
    "\n",
    "    output_data/\n",
    "        <optional: sfr-name>/\n",
    "                <input_data_name>/\n",
    "                    <output_data_name>/\n",
    "                        convolution_result/\n",
    "                            <optional: convolution-result output name>/\n",
    "                                <lookback time or redshift value>\n",
    "\n",
    "Let us go over each of these to understand them:\n",
    "- `output_data/`: Main group-name for the output data.\n",
    "- `<optional: sfr-name>/`: When multiple sfr-dicts are supplied in config['SFR_info'], the results of the convolution for each of these are stored based on the name of that `sfr-dict` in this subtree. **TODO: link to sfr-dict notebook/section.**\n",
    "- `<input_data_name>`: Signifies the `input_data_name`, and depends on what is passed in in the `convolution-instruction`. **TODO: link to convolution-instruction notebook/section.**\n",
    "- `<output_data_name>`: Signifies the `output_data_name`, and depends on what is passed in in the `convolution-instruction`. **TODO: link to convolution-instruction notebook/section.**\n",
    "- `convolution_result`: main sub-group name for the output data.\n",
    "- `<optional: convolution-result output name>/`: Possible when the user returns multiple result dicts (combined in a list) as the output of the post-convolution function. **TODO: link to post-convolution notebook section.**\n",
    "- `<lookback time or redshift value>`: Time-type value of the results of the convolution. \n",
    "\n",
    "Within this group the convolution results are stored, with fields containing the relevant data as subgroups. Any units that are associated to these fields are stored in the 'attrs' of that hdf5-group, and can be read out  as follows:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import astropy.units as u\n",
    "\n",
    "groupname = ... # f\"output_data/event/stochastic_example/stochastic_example/convolution_results/{formation_time_bin_key}\"\n",
    "\n",
    "# convert units\n",
    "unit_dict = json.loads(\n",
    "    output_hdf5_file[\n",
    "        groupname\n",
    "    ].attrs[\"units\"]\n",
    ")\n",
    "unit_dict = {key: u.Unit(val) for key, val in unit_dict.items()}\n",
    "```\n",
    "\n",
    "Different types of input-data also each have somewhat different content stored in their output-group which can be read out as follows:\n",
    "\n",
    "### integration-convolution Event-based output.\n",
    "Output for this type of data, unless supplemented by a post-convolution function, contains just the 'yield' field (i.e. the normalized_yield of the systems times the star formation (rate). This array can directly be attached to the input data dataframes as a column, as it maintains the same order and length.\n",
    "\n",
    "### Sampling-convolution Event-based output.\n",
    "Output of this type of data, unless supplemented by a post-convolution function, contains the `indices` field, which can be used to retrieve the input-data, as well as a `sampled_formation_lookback_times` which contains assigned formation lookback-times. **TODO: what about redshifts?**\n",
    "\n",
    "**TODO: explain multiple sfr-structure**\n",
    "**TODO: explain multiple convolution-result structure**\n",
    "**TODO: explain how**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeee41-da60-49d3-a9de-e690a7e5897f",
   "metadata": {},
   "source": [
    "Lets have a look in the example convolution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff39d57-aa67-4e22-adc4-85d6b472388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['RLOF']>\n",
      "<KeysViewHDF5 ['RLOF_rate']>\n",
      "<KeysViewHDF5 ['convolution_results']>\n",
      "<KeysViewHDF5 ['0.5 yr', '1.5 yr']>\n",
      "<KeysViewHDF5 ['yield']>\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_hdf5_filename, 'r') as output_hdf5_file:\n",
    "    print(output_hdf5_file['output_data/'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results/0.5 yr'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results/0.5 yr/yield'][()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb3aa6-9828-46a3-bab9-9896371a6698",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Descriptions of convolution configuration options and convolution instruction options\n",
    "\n",
    "Both the convolution configuration and the convolution instructions have many options. These are all visible in `default_convolution_config_descriptions` and `default_convolution_instruction_descriptions`, and available (latest release) on https://synthetic-stellar-pop-convolve.readthedocs.io/en/latest/default_convolution_config.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6158b58a-a3cc-4d96-bdfd-751c144dad80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from syntheticstellarpopconvolve.default_convolution_config import default_convolution_config_descriptions\n",
    "from syntheticstellarpopconvolve.default_convolution_instruction import default_convolution_instruction_descriptions\n",
    "\n",
    "# for key in sorted(default_convolution_config_descriptions.keys()):\n",
    "#     print(\"Key: {}\\nDescription: {}\\n\".format(key, default_convolution_config_descriptions[key]))\n",
    "\n",
    "# for key in sorted(default_convolution_instruction_descriptions.keys()):\n",
    "#     print(\"Key: {}\\nDescription: {}\\n\".format(key, default_convolution_instruction_descriptions[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178658bb-fb4c-495b-a101-b9e121838ebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Complete convolution script\n",
    "An example of a complete script is provided below. We will make use of the example data and convolve both event-based data and ensemble-based data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "856300e9-34f8-4938-b83d-94ddfa52d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['RLOF']>\n",
      "<KeysViewHDF5 ['RLOF_rate']>\n",
      "<KeysViewHDF5 ['convolution_results']>\n",
      "<KeysViewHDF5 ['0.5 yr', '1.5 yr', '2.5 yr', '3.5 yr']>\n",
      "<KeysViewHDF5 ['yield']>\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import h5py\n",
    "import pkg_resources\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "\n",
    "from syntheticstellarpopconvolve import convolve, default_convolution_config, default_convolution_instruction\n",
    "from syntheticstellarpopconvolve.general_functions import temp_dir\n",
    "\n",
    "TMP_DIR = temp_dir(\"notebooks\", \"notebook_convolution\", clean_path=True)\n",
    "\n",
    "# create file\n",
    "output_hdf5_filename = os.path.join(TMP_DIR, \"output_hdf5.h5\")\n",
    "generate_boilerplate_outputfile(output_hdf5_filename)\n",
    "\n",
    "# load RLOF data as pandas dataframe\n",
    "example_RLOF_events_filename = pkg_resources.resource_filename(\n",
    "    \"syntheticstellarpopconvolve\", 'example_data/example_RLOF_event_data.dat'\n",
    ")\n",
    "RLOF_df = pd.read_csv(example_RLOF_events_filename, header=0, sep=\"\\s+\")\n",
    "\n",
    "# store the dataframe in the hdf5file\n",
    "RLOF_df.to_hdf(\n",
    "    output_hdf5_filename, key=\"input_data/RLOF\"\n",
    ")\n",
    "\n",
    "#################\n",
    "# Global configuration\n",
    "convolution_config = copy.copy(default_convolution_config)\n",
    "convolution_config['output_filename'] = os.path.join(TMP_DIR, 'output_hdf5.h5')\n",
    "convolution_config['tmp_dir'] = TMP_DIR\n",
    "convolution_config[\"convolution_lookback_time_bin_edges\"] = (\n",
    "    np.array([0, 1, 2, 3, 4]) * u.yr\n",
    ")\n",
    "\n",
    "# convolution instructions\n",
    "convolution_config['convolution_instructions'] = [\n",
    "    {\n",
    "        **default_convolution_instruction,\n",
    "        'input_data_name': 'RLOF',\n",
    "        'output_data_name': 'RLOF_rate',\n",
    "        'data_column_dict': {\n",
    "            # required\n",
    "            'delay_time': 'initial_time',\n",
    "            'normalized_yield': 'probability',\n",
    "            # # optional*\n",
    "            # 'metallicity': 'metallicity',\n",
    "            # optional**\n",
    "            'mass_1': 'initial_mass_accretor',\n",
    "            'mass_2': 'initial_mass_donor',\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up SFR\n",
    "convolution_config[\"SFR_info\"] = {\n",
    "    \"lookback_time_bin_edges\": np.array([0, 1, 2, 3, 4, 5]) * u.yr,\n",
    "    \"starformation_rate_array\": np.array([1, 1, 1, 1, 1]) * u.Msun / u.yr,\n",
    "}\n",
    "\n",
    "# convolve\n",
    "convolve(config=convolution_config)\n",
    "\n",
    "# Show some of the content\n",
    "with h5py.File(convolution_config['output_filename'], 'r') as output_hdf5_file:\n",
    "    print(output_hdf5_file['output_data/'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results/0.5 yr'].keys())\n",
    "    print(output_hdf5_file['output_data/RLOF/RLOF_rate/convolution_results/0.5 yr/yield'][()])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
